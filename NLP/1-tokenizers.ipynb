{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface의 Tokenizers : 정규화와 사전토큰화 제공\n",
    "\n",
    "### 정규화(Normalization)\n",
    "- 일관된 형식으로 텍스트 표준화\n",
    "- 모호한 경우를 방지 -> 일부 문자를 대체하거나 제거\n",
    "- 불필요한 공백 제거, 대소문자 변환, 유닠드 정규화, 구두점 처리, 특수문자 처리 등\n",
    "\n",
    "\n",
    "### 사전토큰화(Pre-tokenization)\n",
    "- 입력 문장을 토큰화하기 전에 단어와 같은 작은 단위로 나누는 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 정규화 및 사전토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordPiece())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 : NFD 유니코드 정규화, 소문자 변환\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase()])\n",
    "\n",
    "# 사전토큰화 : 공백, 구두점 기준으로 분리\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "corpus_path = './datasets/corpus.txt'\n",
    "tokenizer.train([corpus_path])\n",
    "\n",
    "# 저장: 정규화, 사전 토큰화 등 메타데이터와 함께 어휘 사전에 저장\n",
    "tokenizer.save(\"./models/1.petition_wordpiece.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file('./models/1.petition_wordpiece.json')\n",
    "tokenizer.decoder = WordPieceDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 토크나이저를 학습했습니다. -> ['안녕하세요', ',', '토', '##크', '##나이', '##저', '##를', '학습', '##했습니다', '.']\n",
      "['안녕하세요', '토크나이저는 자연어 전처리 방법입니다.', '다음은 임베딩입니다.'] -> [['안녕하세요'], ['토', '##크', '##나이', '##저는', '자연', '##어', '전', '##처리', '방법입니다', '.'], ['다음은', '임', '##베', '##딩', '##입니다', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"안녕하세요, 토크나이저를 학습했습니다.\"\n",
    "sentences = [\"안녕하세요\", \"토크나이저는 자연어 전처리 방법입니다.\", \"다음은 임베딩입니다.\"]\n",
    "\n",
    "encoded_sentence = tokenizer.encode(sentence)\n",
    "encoded_sentences = tokenizer.encode_batch(sentences)\n",
    "\n",
    "print(sentence, \"->\" , encoded_sentence.tokens)\n",
    "print(sentences, \"->\" , [enc.tokens for enc in encoded_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 토크나이저를 학습했습니다. -> [8760, 11, 8693, 8415, 16269, 7536, 7510, 15016, 8315, 13]\n",
      "['안녕하세요', '토크나이저는 자연어 전처리 방법입니다.', '다음은 임베딩입니다.'] -> [[8760], [8693, 8415, 16269, 23335, 9969, 7497, 7615, 10152, 25450, 13], [23330, 7972, 9139, 11171, 7598, 13]]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩\n",
    "print(sentence, \"->\" , encoded_sentence.ids)\n",
    "print(sentences, \"->\" , [enc.ids for enc in encoded_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8760, 11, 8693, 8415, 16269, 7536, 7510, 15016, 8315, 13] -> 안녕하세요, 토크나이저를 학습했습니다.\n",
      "[[8760], [8693, 8415, 16269, 23335, 9969, 7497, 7615, 10152, 25450, 13], [23330, 7972, 9139, 11171, 7598, 13]] -> ['안녕하세요', '토크나이저는 자연어 전처리 방법입니다.', '다음은 임베딩입니다.']\n"
     ]
    }
   ],
   "source": [
    "# 디코딩\n",
    "decoded_sentence = tokenizer.decode(encoded_sentence.ids)\n",
    "decoded_sentences = [tokenizer.decode(enc.ids) for enc in encoded_sentences]\n",
    "\n",
    "\n",
    "print(encoded_sentence.ids, \"->\" , decoded_sentence)\n",
    "print([enc.ids for enc in encoded_sentences], \"->\" , [dnc for dnc in decoded_sentences])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference\n",
    "- 파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터비전 심층학습(위키북스, 윤대희, 김동화, 송종민, 진현두 지음, 2023)<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('book')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbb9e52b27e1643625a5b3f8a7047453142bd39b43540a1ee4af44f044e42c0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
