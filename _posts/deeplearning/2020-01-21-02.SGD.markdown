---
layout: post
title:  "bias meaning in y = ax + b"
img: china_tiger_leaping_gorge.jpg
date:   2020-01-20 00:00:00 +0200
description: Curabitur vulputate enim at metus malesuada, in iaculis nisl tincidunt. Mauris dapibus ut ante ornare ullamcorper. Vivamus ultrices erat lorem. Phasellus pretium nisl ac elit porttitor, id condimentum velit aliquam.
categories: DeepLearning
permalink: '/deeplearning/'
---

# SGD의 배치 size


#####  | keras API를 이용하다보면 모델을 학습하는 부분에 이런 코드가 나온다.

```
model.compile(loss = 'binary_crossentropy', optimizer= 'SGD')
model.fit(X, y, batch_size = 32, epochs = 20, verbose = 1)

```
지금까지 이해하기로는
만약 데이터가 1000개라면 1000개를 32개씩 나누어 학습시키고  loss를 계산할 떼 32개 데이터의 평균을 이용해서 weight를 업데이트 한다고 생각했는데...
optimizer를 GD라고 한다면 내가 생각한 개념이 맞는데 찾아보니 keras에 optimizer 옵션에는 'GD'가 없다..
[keras_optimizers](https://keras.io/optimizers/#sgd)
SGD( Stochastic gradient descent)는 통계적이라는 의미가 있으니...어떤 의미인지 알아보고 싶어졌다


다양한 블로그를 보면..
batch size= 1일 때 데이터 중 1개를 랜덤으로 뽑아서 weight를 업데이트한다는데..
그럼 만약 데이터가 1000개이고 batch size = 1 이라면 1개만 학습에 사용하고, 나머지 999개는 사용하지 않는다는 말인가? 그럼 데이터가 많은 의미가 있는걸까?

그래서 이전에 공부하면서 작성한 블로그를 다시 찾아보니..
[SGD_blog](https://blog.naver.com/wildgrapes18/221550519186)

---
#### GD: full batch 경사하강법
데이터가 1000개라면 loss를 구할 때 1000개의 데이터에 대해 오차를 구할 때 제곱 연산을 1000번해야 하니 시간도 오래 걸리고, 계산량도 너무 많다 (epoch까지 고려한다면 정말 많을 것 같다)

#### SGD: mini batch 경사하강법
데이터가 1000개이고 batch size = 100, epoch = 1000이라면라면
데이터를 대표하는 샘플 32개에 대해서만 loss를 계산하면 되므로 계산량이 적고, 빠름
- 1000(epoch) * 10 (데이어 수/ batch size) = 10000번 계산 -> 더 빠름
- 샘플을 추출하기 때문에 모집단에 대해서는 오차가 있을 수밖에 없음 -> 지그재그 현상이 발생하지만, 이 것이 로컬 미니마에 빠지는 것을 막아주기도 함.
- 시간을 단축시킬 수 있고, 전체 데이터를 학습하는 것만큼 성능을 내기 때문에SGD를 사용함.

---
## 배치학습 vs online 학습 vs offline학습 
같은 batch라는 용어가 나와서 헛갈려서 다시 찾아본 개념..

[배치학습과 온라인 학습](https://stickie.tistory.com/44)



